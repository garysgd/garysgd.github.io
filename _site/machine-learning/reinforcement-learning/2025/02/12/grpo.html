<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reinforcement Learning for Large Language Models | if: Learn</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Reinforcement Learning for Large Language Models" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The introduction of reasoning methods marks a paradigm shift in the way computation is performed using large language models. Reasoning was first introduced into the mainstream with ChatGPT o1, where reasoning tokens were generated as part of the output. The more tokens were generated, the higher quality the model would perform in benchmarks, in particular those related to math and coding. This also has a nice analogy/mapping back to the classic space-time tradeoff for computing. With a high amount of storage, less compute is needed to solve a problem. If computing can be done on the fly, then less storage is needed. This is also apt in another sense, since pretraining neural networks has also been shown as a compression/storage technique. The prompts/inputs to the LLM can be thought of as a key or unique identifier, and when paired with the compressed LLM it is able to decompress a lossy version of the training data." />
<meta property="og:description" content="The introduction of reasoning methods marks a paradigm shift in the way computation is performed using large language models. Reasoning was first introduced into the mainstream with ChatGPT o1, where reasoning tokens were generated as part of the output. The more tokens were generated, the higher quality the model would perform in benchmarks, in particular those related to math and coding. This also has a nice analogy/mapping back to the classic space-time tradeoff for computing. With a high amount of storage, less compute is needed to solve a problem. If computing can be done on the fly, then less storage is needed. This is also apt in another sense, since pretraining neural networks has also been shown as a compression/storage technique. The prompts/inputs to the LLM can be thought of as a key or unique identifier, and when paired with the compressed LLM it is able to decompress a lossy version of the training data." />
<link rel="canonical" href="http://localhost:4000/machine-learning/reinforcement-learning/2025/02/12/grpo.html" />
<meta property="og:url" content="http://localhost:4000/machine-learning/reinforcement-learning/2025/02/12/grpo.html" />
<meta property="og:site_name" content="if: Learn" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-02-12T00:00:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Reinforcement Learning for Large Language Models" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-02-12T00:00:00+08:00","datePublished":"2025-02-12T00:00:00+08:00","description":"The introduction of reasoning methods marks a paradigm shift in the way computation is performed using large language models. Reasoning was first introduced into the mainstream with ChatGPT o1, where reasoning tokens were generated as part of the output. The more tokens were generated, the higher quality the model would perform in benchmarks, in particular those related to math and coding. This also has a nice analogy/mapping back to the classic space-time tradeoff for computing. With a high amount of storage, less compute is needed to solve a problem. If computing can be done on the fly, then less storage is needed. This is also apt in another sense, since pretraining neural networks has also been shown as a compression/storage technique. The prompts/inputs to the LLM can be thought of as a key or unique identifier, and when paired with the compressed LLM it is able to decompress a lossy version of the training data.","headline":"Reinforcement Learning for Large Language Models","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine-learning/reinforcement-learning/2025/02/12/grpo.html"},"url":"http://localhost:4000/machine-learning/reinforcement-learning/2025/02/12/grpo.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="if: Learn" /><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"></script>

</head>
<body><header class="site-header" role="banner">
  <div class="wrapper"><a class="site-title" rel="author" href="/">if: Learn</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 
                h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 
                c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 
                C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Reinforcement Learning for Large Language Models</title>

    <!-- If you want custom highlighting, link it here or in default.html -->
    <link rel="stylesheet" href="/assets/css/custom-highlight.css">
  </head>

  <body>
    <article class="post">
      <!-- Title or other info if needed -->
      <header class="post-header">
        <h1>Reinforcement Learning for Large Language Models</h1>
      </header>

      <div class="post-content">
        <p>The introduction of reasoning methods marks a paradigm shift in the way computation is performed using large language models. 
Reasoning was first introduced into the mainstream with ChatGPT o1, where reasoning tokens were generated as part of the output.
The more tokens were generated, the higher quality the model would perform in benchmarks, in particular those related to math and coding.
This also has a nice analogy/mapping back to the classic space-time tradeoff for computing.
With a high amount of storage, less compute is needed to solve a problem.
If computing can be done on the fly, then less storage is needed.
This is also apt in another sense, since pretraining neural networks has also been shown as a compression/storage technique.
The prompts/inputs to the LLM can be thought of as a key or unique identifier, and when paired with the compressed LLM it is able to decompress a lossy version of the training data.</p>

<p>While previous LLMs could reason to some extent using basic prompt engineering techniques like ‘think step by step’, o1 was one of the first models where reasoning was part of training the model. A further breakthrough came with the introduction of deepseek r1, a reasoning model with open sourced code. The launch of r1 caused a huge waves in both the tech sector and the economy, as training costs were reported to be far lower than that of previous LLMs. This resulted in a large drop in market cap for GPU manufacturers that provide hardware for training models.</p>

<p>One of the key innovations of the deepseek model is the Group Relative Policy Optimization (GRPO) algorithm, which removes the need for training the critic network that is typically a part of the Proximal Policy Optimization (PPO) used to train previous LLMs. We will first introduce how PPO works in the context of LLMs before highlighting the innovations of GRPO.</p>

<p>The PPO surrogate objective is given by:
\(J_{\text{PPO}}(\theta) = \mathbb{E}_{q \sim P(Q),\, o \sim \pi_{\theta}^{\text{old}}(\cdot \mid q)}
\left[
\frac{1}{|o|} \sum_{t=1}^{|o|}
\min\!\left\{
\frac{\pi_\theta(o_t \mid q, o_{&lt;t})}{\pi_{\theta}^{\text{old}}(o_t \mid q, o_{&lt;t})} A_t,\,
\text{clip}\!\left(
\frac{\pi_\theta(o_t \mid q, o_{&lt;t})}{\pi_{\theta}^{\text{old}}(o_t \mid q, o_{&lt;t})},\, 1-\epsilon,\, 1+\epsilon
\right) A_t
\right\}
\right].\)</p>

<p>We can first go through this equation term by term in the context of natural language processing to gain better insight on how PPO works. We can start with the policy:</p>

\[\pi_\theta(o_t \mid q, o_{&lt;t})\]

<p>For nlp the policy \(\pi_\theta\) is determined by the neural network, typically based on the transformer architecture. The neural network takes in \(t-1\) input tokens of a sequence \(o\) along with the prompt sequence $q$ to predict the $t$ token of the sequence denoted by \(o_t\). We can illustrate this with a simple example. While tokens typically denote subwords akin to phonemes rather than actual words, we will assume each word is a token for the sake of simplification.</p>

      </div>
    </article>
  </body>
</html>

      </div>
    </main><footer class="site-footer h-card">
    <div class="wrapper">
      <div class="footer-col-wrapper">
        <div class="footer-col">
          <p>Exploring Machine Learning and Theoretical Work | Gary Phua</p>
        </div>
      </div>
    </div>
  </footer></body>

</html>
