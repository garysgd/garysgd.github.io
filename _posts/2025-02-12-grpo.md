---
layout: post
title: "Reasoning and RL for Large Language Models"
subtitle: "From PPO to GRPO"
date: 2025-02-12
categories: [machine-learning, reinforcement-learning]
tags: [python, llms, generative ai]
---

When we first think of reinforcement learning in the context of machine learning, we usually think of a grid with a start and end point. This is the standard reinforcement learning problem of teaching an agent to reach its goal using a clearly defined reward function. While reinforcement learning techniques have been existed for decades, its use for natural language processing (NLP) is fairly more recent, with arguably the most successful use case being to train ChatGPT. It is due to the lack of a clear reward that has led to this time gap between the formulation of such methods and its use to train language models. OpenAI solved this gap in part due to the power of belief and capitalism. The belief that such models can scale, and the capital from investors(at a scale largley unavailable to academics) to hire human evaluators to label data that it would eventually be used in its now well known reinforcement learning with human feedback (RLHF) algorithm.

The introduction of reasoning methods marks another paradigm shift in the way computation is performed using large language models. This has resulted in another form of scaling laws, moving away from improving performance at training time to inference time scaling. To reason is to be able to think logically in a series of sequential steps. Like walking step by step through a grid to get to a goal, reasoning is a natural complement that works well with reinforcement learning. 

Reasoning was first introduced into the mainstream with ChatGPT o1, where reasoning tokens were generated as part of the output. The more tokens were generated, the higher quality the model would perform in benchmarks, in particular those related to math and coding. This also has a nice analogy/mapping back to the classic space-time tradeoff for computing. With a high amount of storage, less compute is needed to solve a problem. If computing can be done on the fly, then less storage is needed. This is also apt in another sense, since pretraining neural networks has also been shown as a compression/storage technique. The prompts/inputs to the LLM can be thought of as a key or unique identifier, and when paired with the compressed LLM it is able to decompress a lossy version of the training data.

Previous LLMs could reason in a adhoc sense using basic prompt engineering techniques like asking the model to 'think step by step'. However, o1 was one of the first models where reasoning was part of training the model. A further breakthrough came with the introduction of deepseek r1, a reasoning model with open sourced code. One of the key innovations of the deepseek model is the Group Relative Policy Optimization (GRPO) algorithm, which removes the need for training the critic network that is typically a part of the Proximal Policy Optimization (PPO) used to train previous LLMs. 

In this post, I will first introduce the various reinforcement learning notations that are used in the context of NLP, followed by going through the classic PPO objective followed by GRPO introduced by deepseek.

## RL Definitions in an NLP Context

Before diving into PPO and GRPO, we first defined the various notations used in the context of reinforcement learning for LLMs:

- **Prompt ($$q$$)**: The initial user input providing context for generation (can be denoted as $$q_t$$ if dynamic).

- **State ($$s_t$$)**: The combination of the prompt and all tokens generated so far, i.e., 
  $$ s_t = (q, o_{<t}) $$

- **Action ($$o_t$$)**: The next token generated by the model at time $$t$$.

- **Reward ($$r_t$$)**: A numerical score indicating how “good” the chosen token or sequence is (can be step-level or sequence-level).

- **Policy ($$\pi_\theta$$)**: The mapping from state $$s_t$$ to a probability distribution over the next token; effectively, the LLM parameterized by $$\theta$$.

- **Discount Factor ($$\gamma$$)**: Weights future rewards relative to immediate ones. For example, with $$\gamma=0.9$$, rewards are discounted by 10% per time step.

- **GAE Parameter ($$\lambda$$)**: Balances bias and variance in advantage estimation by controlling how much future rewards influence the advantage calculation (higher $$\lambda$$ includes more future rewards).


## Proximal Policy Optimization

The PPO surrogate objective is given by:
$$
J_{\text{PPO}}(\theta) = \mathbb{E}_{q \sim P(Q),\, o \sim \pi_{\theta}^{\text{old}}(\cdot \mid q)}
\left[
\frac{1}{|o|} \sum_{t=1}^{|o|}
\min\!\left\{
\frac{\pi_\theta(o_t \mid q, o_{<t})}{\pi_{\theta}^{\text{old}}(o_t \mid q, o_{<t})} A_t,\,
\text{clip}\!\left(
\frac{\pi_\theta(o_t \mid q, o_{<t})}{\pi_{\theta}^{\text{old}}(o_t \mid q, o_{<t})},\, 1-\epsilon,\, 1+\epsilon
\right) A_t
\right\}
\right].
$$

We can first go through this equation term by term in the context of natural language processing to gain better insight on how PPO works. We can start with the policy:

$$\pi_\theta(o_t \mid q, o_{<t})$$

For NLP the policy $$\pi_\theta$$ is determined by the neural network, typically based on the transformer architecture. The neural network takes in $$t-1$$ input tokens of a sequence $$o$$ along with the prompt sequence $$q$$ to predict the $$t$$ token of the sequence denoted by $$o_t$$. We can illustrate this with a simple example. While tokens typically denote subwords akin to phonemes rather than actual words, we will assume each word is a token for the sake of simplification.

Given a prompt $$q$$ such as "What is the capital of France?" the policy model $$\pi_\theta$$ starts to predict starting with the first token $$o_1$$ which could be "The". The policy model would then output subsequent tokens dependent based on the prompt and the previous output tokens. The ratio $$\frac{\pi_\theta(o_t \mid q, o_{<t})}{\pi_{\theta}^{\text{old}}(o_t \mid q, o_{<t})}$$ then measures how much more likely the new policy is to predict the subsequent token $$o_t$$ relative to the old policy. It acts as a weight for the advantage $$A_t$$ which we will now elaborate on.

For PPO, the advantage $$A_t$$ can be expressed as 

$$A_t = \sum_{l=0}^{T-t-1} (\gamma \lambda)^l \left[ Q(s_{t+l}, o_{t+l}) - V(s_{t+l}) \right]$$

$$Q(s_t, o_t)$$ represents the cumulative reward after outputting $$o_t$$. It is the sum of the immediate reward for predicting that token denoted by $$r_t$$ and the discounted sum of future rewards determined by the value network.

$$Q(s_t, o_t) \approx r_t + \gamma\, V(s_{t+1})$$

The value network itself is trained using the loss function:

$$ L_V=\frac{1}{2}\,\mathbb{E}_t\left[\left(V_\psi(s_t)-R_t\right)^2\right]$$

Where $$R_t$$ is the true expected sum of the discounted future rewards:


$$R_t = r_t + \gamma\, r_{t+1} + \gamma^2\, r_{t+2} + \cdots + \gamma^{T-t}\, r_T.$$

From these definitions, advantage quantifies the difference between the cumulative reward received for the current action (which in the case of NLP is the token generated) and the reward that was expected according to the value network. A positive advantage means that the token performed better than expected, while a negative advantage indicates it performed worse. 

Intuitively we can see that having a advantage means that the policy is outputting a better token than expected which is what we want to optimize. Going back to our equation for PPO, advantage plays is an part in optimizing the PPO objective which is an expectation of two terms. 

The purpose of the first term, $$\frac{\pi_\theta(o_t \mid q, o_{<t})}{\pi_{\theta}^{\text{old}}(o_t \mid q, o_{<t})} A_t$$ is to prevent the new policy $$\pi_\theta(o_t \mid q, o_{<t})$$ from deviating too much from the old policy $${\pi_{\theta}^{\text{old}}(o_t \mid q, o_{<t})}$$. This prevents unpredictable behaviour of the policy and also prevent it from forgetting the properties of the old policy which may have merit.

The second term $$\text{clip}\left(\frac{\pi_\theta(o_t \mid q, o_{<t})}{\pi_{\theta}^{\text{old}}(o_t \mid q, o_{<t})},\, 1-\epsilon,\, 1+\epsilon\right) \, A_t$$ prevents any token from being overly rewarded or having a high value. Without clipping the policy model may be overly biased towards certain tokens.

Having now gone through all the terms, we can now describe the proximal policy objective as training a network to optmise the next token culmulative reward with some constrants.