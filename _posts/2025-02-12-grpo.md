---
layout: post
title: "Reasoning and RL for Large Language Models"
subtitle: "From PPO to GRPO"
date: 2025-02-12
categories: [machine-learning, reinforcement-learning]
tags: [python, llms, generative ai]
---

When we first think of reinforcement learning in the context of machine learning, we usually think of a grid with a start and end point. This is the standard reinforcement learning problem of teaching an agent to reach its goal using a clearly defined reward function. While reinforcement learning techniques have been existed for decades, its use for natural language processing (NLP) is fairly more recent, with arguably the most successful use case being to train ChatGPT. It is due to the lack of a clear reward that has led to this time gap between the formulation of such methods and its use to train language models. OpenAI solved this gap in part due to the power of belief and capitalism. The belief that such models can scale, and the capital from investors(at a scale largley unavailable to academics) to hire human evaluators to label data that it would eventually be used in its now well known reinforcement learning with human feedback (RLHF) algorithm.

The introduction of reasoning methods marks another paradigm shift in the way computation is performed using large language models. This has resulted in another form of scaling laws, moving away from improving performance at training time to inference time scaling. To reason is to be able to think logically in a series of sequential steps. Like walking step by step through a grid to get to a goal, reasoning is a natural complement that works well with reinforcement learning. 

Reasoning was first introduced into the mainstream with ChatGPT o1, where reasoning tokens were generated as part of the output. The more tokens were generated, the higher quality the model would perform in benchmarks, in particular those related to math and coding. This also has a nice analogy/mapping back to the classic space-time tradeoff for computing. With a high amount of storage, less compute is needed to solve a problem. If computing can be done on the fly, then less storage is needed. This is also apt in another sense, since pretraining neural networks has also been shown as a compression/storage technique. The prompts/inputs to the LLM can be thought of as a key or unique identifier, and when paired with the compressed LLM it is able to decompress a lossy version of the training data.

Previous LLMs could reason in a adhoc sense using basic prompt engineering techniques like asking the model to 'think step by step'. However, o1 was one of the first models where reasoning was part of training the model. A further breakthrough came with the introduction of deepseek r1, a reasoning model with open sourced code. One of the key innovations of the deepseek model is the Group Relative Policy Optimization (GRPO) algorithm, which removes the need for training the critic network that is typically a part of the Proximal Policy Optimization (PPO) used to train previous LLMs. 

In this post, I will first introduce the various reinforcement learning notations that are used in the context of NLP, followed by going through the classic PPO objective followed by GRPO introduced by deepseek.

## RL Definitions in an NLP Context

Before diving into PPO and GRPO, we first defined the various notations used in the context of reinforcement learning for LLMs:

- **Prompt ($$q$$)**: The initial user input providing context for generation (can be denoted as $$q_t$$ if dynamic).

- **State ($$s_t$$)**: The combination of the prompt and all tokens generated so far, i.e., 
  $$ s_t = (q, o_{<t}) $$

- **Action ($$o_t$$)**: The next token generated by the model at time $$t$$.

- **Reward ($$r_t$$)**: A numerical score indicating how “good” the chosen token or sequence is (can be step-level or sequence-level).

- **Policy ($$\pi_\theta$$)**: The mapping from state $$s_t$$ to a probability distribution over the next token; effectively, the LLM parameterized by $$\theta$$.

- **Discount Factor ($$\gamma$$)**: Weights future rewards relative to immediate ones. For example, with $$\gamma=0.9$$, rewards are discounted by 10% per time step.

- **GAE Parameter ($$\lambda$$)**: Balances bias and variance in advantage estimation by controlling how much future rewards influence the advantage calculation (higher $$\lambda$$ includes more future rewards).


## Proximal Policy Optimization

The PPO surrogate objective is given by:

$$
\begin{aligned}
J_{\text{PPO}}(\theta)
&= \mathbb{E}_{q \sim P(Q),\, o \sim \pi_{\theta}^{\text{old}}(\cdot \mid q)}
\Biggl[
  \frac{1}{|o|} \sum_{t=1}^{|o|}
  \min\Bigl\{
    \frac{\pi_\theta(o_t \mid q,\,o_{<t})}{\pi_{\theta}^{\text{old}}(o_t \mid q,\,o_{<t})} \, A_t,\; \\[1mm]
&\quad
    \text{clip}\Bigl(
      \frac{\pi_\theta(o_t \mid q,\,o_{<t})}{\pi_{\theta}^{\text{old}}(o_t \mid q,\,o_{<t})},\; 1-\epsilon,\; 1+\epsilon
    \Bigr)\,A_t
  \Bigr\}
\Biggr]
\end{aligned}
$$


We can first go through this equation term by term in the context of natural language processing (NLP) to gain better insight into how PPO works. We start with the policy:

$$
\pi_\theta(o_t \mid q, o_{<t})
$$

For NLP, the policy $$\pi_\theta$$ is determined by a neural network (typically based on the transformer architecture). The network takes in $$t-1$$ input tokens of the sequence $$o$$ along with the prompt $$q$$ to predict the $$t^{th}$$ token, denoted by $$o_t$$. For simplicity, we assume each word is a token.

Given a prompt $$q$$ such as "What is the capital of France?", the policy model $$\pi_\theta$$ begins by predicting the first token $$o_1$$, which could be "The". The model then generates subsequent tokens based on the prompt and the previously generated tokens. The ratio

$$
\frac{\pi_\theta(o_t \mid q, o_{<t})}{\pi_{\theta}^{\text{old}}(o_t \mid q, o_{<t})}
$$

measures how much more likely the new policy is to predict the token $$o_t$$ compared to the old policy. This ratio acts as a weight for the advantage $$A_t$$, which we elaborate on next.

For PPO, the advantage $$A_t$$ can be expressed as

$$
A_t = \sum_{l=0}^{T-t-1} (\gamma \lambda)^l \left[ Q(s_{t+l}, o_{t+l}) - V(s_{t+l}) \right]
$$

Here, $$Q(s_t, o_t)$$ represents the cumulative reward after outputting $$o_t$$. It is the sum of the immediate reward for predicting that token, denoted by $$r_t$$, and the discounted sum of future rewards determined by the value network. In practice, we approximate

$$
Q(s_t, o_t) \approx r_t + \gamma\, V(s_{t+1})
$$

Thus, the temporal-difference error is given by

$$
\delta_t = Q(s_t, o_t) - V(s_t) = r_t + \gamma\, V(s_{t+1}) - V(s_t)
$$

The value network itself is trained using the loss function:

$$
L_V = \frac{1}{2}\,\mathbb{E}_t\left[\left(V_\psi(s_t)-R_t\right)^2\right],
$$

where $$R_t$$ is the true expected sum of the discounted future rewards:

$$
R_t = r_t + \gamma\, r_{t+1} + \gamma^2\, r_{t+2} + \cdots + \gamma^{T-t}\, r_T.
$$

From these definitions, the advantage quantifies the difference between the cumulative reward received for the current action (which in the case of NLP is the token generated) and the reward that was expected according to the value network. A positive advantage means that the token performed better than expected, while a negative advantage indicates it performed worse.

Intuitively, having a positive advantage implies that the policy is outputting a better token than expected, which is what we want to optimize. Returning to our PPO equation, the advantage is used in two terms:

- The first term,
  
  $$
  \frac{\pi_\theta(o_t \mid q, o_{<t})}{\pi_{\theta}^{\text{old}}(o_t \mid q, o_{<t})} A_t,
  $$

  ensures that the new policy does not deviate too much from the old policy, preserving the beneficial properties of the old policy.

- The second term,

  $$
  \text{clip}\!\left(\frac{\pi_\theta(o_t \mid q, o_{<t})}{\pi_{\theta}^{\text{old}}(o_t \mid q, o_{<t})},\, 1-\epsilon,\, 1+\epsilon\right) A_t,
  $$

  prevents any single token from being overly rewarded by capping the ratio, ensuring that updates remain stable and do not lead to drastic changes.

Together, these terms form the PPO surrogate objective, which trains the network to optimize the cumulative reward for the next token while enforcing constraints that prevent large deviations from the previous policy. We can next move on to describe Group Relative Policy Optimization (GRPO) which was introduced by DeepSeek in their DeepSeekMath paper and how it improves upon PPO.

## Group Relative Policy Optimization

The GRPO surrogate objective is given by:

$$
\begin{aligned}
\mathcal{J}_{\text{GRPO}}(\theta)
&= \mathbb{E}_{\substack{q \sim P(Q), \\ \{o_i\}_{i=1}^{G} \sim \pi_{\theta}^{\text{old}}(\cdot \mid q)}}
\Biggl[
  \frac{1}{G} \sum_{i=1}^{G} \frac{1}{|o_i|} \sum_{t=1}^{|o_i|}
  \Biggl\{
    \min \Biggl(
      \frac{\pi_\theta\bigl(o_{i,t} \mid q,\,o_{i,<t}\bigr)}{\pi_{\theta}^{\text{old}}\bigl(o_{i,t} \mid q,\,o_{i,<t}\bigr)}
      \,\hat{A}_{i,t}, \\
&\quad
      \text{clip}\Biggl(
        \frac{\pi_\theta\bigl(o_{i,t} \mid q,\,o_{i,<t}\bigr)}{\pi_{\theta}^{\text{old}}\bigl(o_{i,t} \mid q,\,o_{i,<t}\bigr)},
        1-\epsilon,\,1+\epsilon
      \Biggr)\,\hat{A}_{i,t}
    \Biggr)
    \;-\;
    \beta\, D_{\mathrm{KL}}\bigl[\pi_\theta \,\|\, \pi_{\mathrm{ref}}\bigr]
  \Biggr\}
\Biggr]
\end{aligned}
$$

From the equation above we can see some differences compared to the PPO objective.  There is an additional sum across $$G$$ actions $$o_i$$ drawn from the old policy denoted by 

$$
\{ o_i \}_{i=1}^{G} \sim \pi_{\theta}^{\text{old}}(\cdot \mid q)
$$

which is also expressed as a summation over $$G$$ in the equation. There is also an additional KL divergence term $$D_{KL}$$ which is measures the difference in policy distribution between the new policy and a reference policy, where 

$$
D_{\mathrm{KL}}\!\bigl[\pi_\theta \,\|\, \pi_{\mathrm{ref}}\bigr]
\;=\;
\frac{\pi_{\mathrm{ref}}\bigl(o_{i,t} \mid q,\,o_{i,<t}\bigr)}{\pi_{\theta}\bigl(o_{i,t} \mid q,\,o_{i,<t}\bigr)}
\;-\;
\log \frac{\pi_{\mathrm{ref}}\bigl(o_{i,t} \mid q,\,o_{i,<t}\bigr)}{\pi_{\theta}\bigl(o_{i,t} \mid q,\,o_{i,<t}\bigr)}
\;-\;1.
$$
